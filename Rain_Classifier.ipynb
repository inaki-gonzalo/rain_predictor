{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "name": "Rain Classifier",
      "private_outputs": true,
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "oYM61xrTsP5d"
      },
      "source": [
        "# Rain Classifier\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MfBg1C5NB3X0"
      },
      "source": [
        "<table class=\"tfo-notebook-buttons\" align=\"left\">\n",
        "  <td>\n",
        "    <a target=\"_blank\" href=\"https://colab.research.google.com/drive/1J-AiKNtBcl4ZbYlKNxtkhJ3_Nnr35fgz\"><img src=\"https://www.tensorflow.org/images/colab_logo_32px.png\" />Run in Google Colab</a>\n",
        "  </td>\n",
        "  <td>\n",
        "    <a target=\"_blank\" href=\"https://github.com/inaki-gonzalo/rain_predictor/blob/main/Rain_Classifier.ipynb\"><img src=\"https://www.tensorflow.org/images/GitHub-Mark-32px.png\" />View on GitHub</a>\n",
        "  </td>\n",
        "</table>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "L1otmJgmbahf"
      },
      "source": [
        "## Introduction\n",
        "\n",
        "The goal of this notebook is to build a binary classifier to predict whether it is going to rain.\n",
        "\n",
        "I know nothing about meteorology so I start with raw data from NOAA. It contains precipitation per day along with other variables. I choose the variables that are correlated to precipitation as my features. (See visualization section).\n",
        "\n",
        "Based on a Medium [article](https://medium.com/datadriveninvestor/building-neural-network-using-keras-for-classification-3a3656c726c1).\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bL54LWCHt5q5"
      },
      "source": [
        "## Import libraries"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "dlauq-4FWGZM"
      },
      "source": [
        "from keras import Sequential\n",
        "from keras.layers import Dense\n",
        "from sklearn.metrics import confusion_matrix\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "\n",
        "import matplotlib.pyplot as plt\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import seaborn as sns\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mmaHHH7Pvmth"
      },
      "source": [
        "## Download data\n",
        "\n",
        "Download and clean the data."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "FlsEcKVeuCnf"
      },
      "source": [
        "# Original source of the data.\n",
        "# https://www.ncdc.noaa.gov/cdo-web/search\n",
        "\n",
        "data_columns = ['DATE', 'DailyAverageDewPointTemperature', 'DailyAverageRelativeHumidity',\n",
        "                'DailyPrecipitation', 'DailyHeatingDegreeDays', 'DailyDepartureFromNormalAverageTemperature']\n",
        "data_url = 'https://raw.githubusercontent.com/inaki-gonzalo/rain_predictor/main/sj_data.csv'\n",
        "dataset = pd.read_csv(data_url, sep=',', header=0, parse_dates=[\n",
        "                      'DATE'], usecols=data_columns, na_values=['T'])\n",
        "\n",
        "dataset.dropna(inplace=True)\n",
        "\n",
        "# Convert into correct data types.\n",
        "float_columns = {}\n",
        "for c in data_columns:\n",
        "    if c != 'DATE':\n",
        "        float_columns[c] = np.float16\n",
        "dataset = dataset.astype(float_columns)\n",
        "\n",
        "# Convert precipitation into a binary category.\n",
        "dataset['DailyPrecipitation'] = np.where(\n",
        "    dataset['DailyPrecipitation'] > 0.01, 1, 0)\n",
        "\n",
        "# Make day of year a feature.\n",
        "dataset['dayofyear'] = dataset['DATE'].dt.dayofyear"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yTY8qzyYv3vl"
      },
      "source": [
        "## Visualize\n",
        "\n",
        "Explore relationships between variables in your data. The goal here is to find a picture were the dots of different colors are separate clusters."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "WBtFK1hO8KsO"
      },
      "source": [
        "sns.pairplot(dataset, hue='DailyPrecipitation')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Zh2KZvUTxKvB"
      },
      "source": [
        "For the following I see the row with DailyPrecipitation. I want to choose featues with high correlation."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "NTnQjoxyfsDE"
      },
      "source": [
        "sns.heatmap(dataset.corr(), annot=True)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "o-0PjH0ulbtL"
      },
      "source": [
        "## Create and split data\n",
        "Remove the daily precipitation from the input data and put it as output.\n",
        "\n",
        "The data is all in different units. To make it easier for the model to learn from it, we pass the inputs through a standard scalar.\n",
        "\n",
        "Split the data into train and test.\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "umB5tswsfTEQ"
      },
      "source": [
        "del data_columns[data_columns.index('DATE')]\n",
        "del data_columns[data_columns.index('DailyPrecipitation')]\n",
        "x = dataset.loc[:, data_columns]\n",
        "y = dataset.loc[:, ['DailyPrecipitation']]\n",
        "\n",
        "# standardizing the input features.\n",
        "sc = StandardScaler()\n",
        "x = sc.fit_transform(x)\n",
        "\n",
        "# Split data into train and test.\n",
        "x_train, x_test, y_train, y_test = train_test_split(x, y, test_size=0.7)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FS_gVStowW3G"
      },
      "source": [
        "## Defining the model\n",
        "\n",
        "This simple 3 layer model seems to be sufficient for this problem."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "RaJW3XrPyFiF"
      },
      "source": [
        "classifier = Sequential()\n",
        "classifier.add(Dense(4, activation='relu',\n",
        "                     kernel_initializer='random_normal', input_dim=len(data_columns)))\n",
        "classifier.add(Dense(4, activation='relu', kernel_initializer='random_normal'))\n",
        "classifier.add(Dense(1, activation='sigmoid',\n",
        "                     kernel_initializer='random_normal'))\n",
        "classifier.compile(\n",
        "    optimizer='adam', loss='binary_crossentropy', metrics=['accuracy'])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Ww53yepKljpm"
      },
      "source": [
        "## Training\n",
        "The data is not balanced because it doesn't rain half of the time.\n",
        "So we need to compensate with class_weight."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "50FYNIb1dmJH"
      },
      "source": [
        "class_weight = {0: 1,\n",
        "                1: 15}\n",
        "classifier.fit(x_train, y_train, batch_size=10, epochs=100, class_weight=class_weight)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "u2e5WupIw2N2"
      },
      "source": [
        "## Evaluate the model"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9f3yBUvkd_VJ"
      },
      "source": [
        "eval_model = classifier.evaluate(x_test, y_test)\n",
        "print(eval_model)\n",
        "\n",
        "y_pred = classifier.predict(x_test)\n",
        "y_pred = (y_pred > 0.5)\n",
        "\n",
        "cm = confusion_matrix(y_test, y_pred)\n",
        "print('\\nNumber of samples:{}'.format(len(y_pred)))\n",
        "print('True negatives:{}'.format(cm[0][0]))\n",
        "print('False negatives:{}'.format(cm[1][0]))\n",
        "print('True positives:{}'.format(cm[0][1]))\n",
        "print('False positives:{}'.format(cm[1][1]))"
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}